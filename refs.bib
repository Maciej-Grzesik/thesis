% Check https://en.wikipedia.org/wiki/BibTeX#Entry_types for various types of bibliographic entries

@booklet{bookOetiker2022,
  author    = {T. Oetiker and H. Partl and I. Hyna and E. Schlegl},
  title     = {Nie za krótkie wprowadzenie do systemu {\LaTeXe{}}},
  year      = {2022},
  publisher = {Free Software Foundation, Inc.},
  note      = {\url{https://polish-mirror.evolution-host.com/ctan/info/lshort/polish/lshort-pl.pdf}}
}

@book{bookSurname2020,
  author    = {A. B. Surname and C. D. {Last Name}},
  title     = {Book Title},
  publisher = {Famous University Press},
  address   = {Cambridge, UK},
  year      = {2020}
}

@article{artSurname2022,
  author  = {E. Surname and F. G. Surname},
  title   = {Article title},
  journal = {Journal Name},
  year    = {2022},
  volume  = {10},
  number  = {2},
  pages   = {12--34}
}

@article{malczewska2011,
  author    = {P. Malczewska},
  title     = {Izolacja społeczna osób z uszkodzonym słuchem jako wspólny obszar badań pedagogiki i antropologii},
  journal   = {Pedagogika a etnologia i antropologia kulturowa. Wspólne obszary badań},
  pages     = {128},
  publisher = {Wydawnictwo Uniwersytet Śląski w Katowicach},
  year      = {2011}
}

@misc{vaswani2023attentionneed,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1706.03762}
}

@online{mamczur2020,
  author = {M. Mamczur},
  title  = {Czym jest i jak działa transformer sieć neuronowa?},
  year   = {2020},
  month  = mar,
  url    = {https://miroslawmamczur.pl/czym-jest-i-jak-dziala-transformer-siec-neuronowa/},
  note   = {Dostęp: 23 maj 2024}
}

@book{american2013diagnostic,
  title={Diagnostic and Statistical Manual of Mental Disorders (DSM-5{\textregistered})},
  author={American Psychiatric Association},
  isbn={9780890425572},
  url={https://books.google.pl/books?id=-JivBAAAQBAJ},
  year={2013},
  publisher={American Psychiatric Publishing}
}


@inproceedings{clark-etal-2019-bert,
    title = "What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention",
    author = "Clark, Kevin  and
      Khandelwal, Urvashi  and
      Levy, Omer  and
      Manning, Christopher D.",
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Belinkov, Yonatan  and
      Hupkes, Dieuwke",
    booktitle = "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4828",
    doi = "10.18653/v1/W19-4828",
    pages = "276--286",
    abstract = "Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT{'}s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT{'}s attention.",
}

@Book{jm3,
  author =       "Daniel Jurafsky and James H. Martin",
  title =        "Speech and Language Processing: An Introduction to
                 Natural Language Processing, Computational Linguistics,
                 and Speech Recognition with Language Models",
  year =         "2024",
  url = {https://web.stanford.edu/~jurafsky/slp3/},
  note = "Online manuscript released August 20, 2024",
}

@article{DBLP:journals/corr/abs-2010-11929,
  author       = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  Dirk Weissenborn and
                  Xiaohua Zhai and
                  Thomas Unterthiner and
                  Mostafa Dehghani and
                  Matthias Minderer and
                  Georg Heigold and
                  Sylvain Gelly and
                  Jakob Uszkoreit and
                  Neil Houlsby},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  journal      = {CoRR},
  volume       = {abs/2010.11929},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.11929},
  eprinttype    = {arXiv},
  eprint       = {2010.11929},
  timestamp    = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-11929.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}